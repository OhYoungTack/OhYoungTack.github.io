#AlexNet(2012)
<img src="/images/AlexNet.png" width="500" height="200">

AlexNet는 2개의 GPU를 기반으로 한 병렬 구조이다. 이는 GPU의 성능이 부족하였기 때문에 이러한 문제를 해결하기 위한 방안이였다. 
구조를 확인해보면 5개의 convolution layers와 3개의 fc(fully-connected) layers로 구성되어있다.
LSVRC는 이미지를 1000개의 다른 클래스로 분류해주어야 하는데 이를 위해 마지막 fc layers는 활성 함수로 softmax함수를 사용하였다.

----

###ReLU
지금은 기본적으로 사용하지만 tanh등을 사용할 시기였을 때, ReLU를 사용함으로써 학습 속도를 개선하였다.

###LRN(Local Response Normalization)
output에서 convolution feature map이 나오면 convolution feature map중에서 일정 부분만 높게 activation되게 해주는 작업을 수행해 주었다.

###Regularization
-Data augmentation

AlexNet에서는 256 *
256의 original Image를 224 * 224의 Smaller Patch로 떠서 하나의 이미지 안에서 데이터를 32 * 32개를 늘릴 수 있고, 좌우 반전까지 하면 그의 *2배를 해주어 총 2048배의 데이터를 늘릴 수 있다.

-Dropout

Dropout은 Network의 일부를 생략하여 다른 Model을 학습한 것과 같은 효과를 얻어 overfitting을 개선하는 방법이다. AlexNet에서는 test할 때 모든 뉴런을 사용하지만, output에 0.5를 곱해주었다. 이 방법은 처음이자 마지막으로 수행되었다고 한다.

#ZFNet(2013)
<img src="/images/ZFNet.png" width="500" height="200">

ZFNet의 구조는 AlexNet에서 GPU를 하나만 쓰고 일부 convolution layer의 kernel 사이즈와 stride를 일부 조절해주었다.(layers1의 filter 사이즈를 11 * 11 -> 7 * 7, stride를 4 -> 1로 수정해주었고, 이에 따라 layers2의 stride를 1 -> 2로 수정하였다.)
ZFNet은 CNN을 가시화하여 CNN의 중간 과정을 눈으로 보고 개선 방향을 파악할 수 있는 방법을 만들었다.(Visualizing)

###Visualizing
CNN은 convolution계산 후 활성 함수를 통해 feature map을 생성하고, pooling하여 이미지를 축소 시키는 것을 반복하는데 이 과적을 반대로 해주어 원본 이미지에 mapping할 수 있는 이미지를 생성 할 수 있을 것이다.<img src="/images/ZFNet_1.png" width="250" height="350">

max-pooling은 이미지를 축소 시키는 과정에서 가장 강한 자극을 전달하는데 이 과정을 반대로하면 어느 부분이 강한 자극인지를 알 수 없는 문제가 생긴다. Visualizing은 가장 강한 자극의 위치를 가지고 있도록 하여 이와 같은 문제를 해결하였다.#VGG(2014)
stride는 모두 1, 제로패딩 1, 모든 convolution filter의 크기는 3 * 3 pooling은 2 * 2을 사용해주었다.

간단한 방법론으로 좋은 성적을 얻었다.

#GoogLeNet(2014)
<img src="/images/GoogLeNet.png" width="300" height="200">

Filter concatenation은 filter를 채널방향으로 더한 것을 의미한다. 

(b)그림은 (a)모델의 개선된 모델이다. 채널의 수를 중간에 한 번 줄였을 때 layers가 오히려 하나 더 쓰였는데 이 네트워크를 정의하는 파라미터의 수가 줄어든다. 이는 convolution의 파라미터를 정의하는 것은 input채널과 output채널이기 때문이다.

Receptive field : 출력단에 있는 하나의 픽셀이 입력 이미지의 얼마의 픽셀을 보고 이 정보가 취합되었는지를 의미한다.

Max-Pooling의 경우 1 * 1이 뒤에있는데 이는 출력 채널의 크기를 맞추기 위함이다. 

#ResNet(2015)
<img src="/images/GoogLeNet.png" width="100" height="300">

network가 deep 할 때 학습이 잘되는가를 생각해보면 아니라는 것을 알 수 있다. 이는 Vanishing/exploding gradients때문인데 이는 Better initialization methods/ batch normalization/ ReLU 과 같은 방법으로 해결할 수 있다.

Overfitting는 Training 에러는 줄어들고 Training acc는 늘어나는데 test acc가 계속 떨어질경우를 말한다. ResNet은 트레이닝도 잘되고 테스트도 잘되는데 성능이 잘 안나오는 경우인 Degradation을 해결하려하였다.

###Residual learning building block어떤 타겟과 현재 입력과의 차이만을 학습하게 해주는 방법이다. 입력과 출력의 차원이 같아야 한다는 단점을 갖고있다. 

1 * 1 conv를 이용하여 채널을 줄여주고 3 * 3 conv를 해주고 다시 1 * 1 conv를 해준다. 마지막에 다시 1 * 1 conv를 해주는 이유는 입력과 출력의 채널을 같게 해주어야하기 때문에 추가해준것이다. 만약 3 * 3 conv의 채널을  입력 채널과 같게해주면 바로 더해줄 수는 있어서 마지막의 1 * 1 conv가 필요없지만 파라미터의 수가 많아질 것이다.

ResNet의 한계는 Layers의 개수가 1000개를 넘어가면 여전히 잘 학습이 안되는 성능을 보인다는 것이다.